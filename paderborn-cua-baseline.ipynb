{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline (Source-only with .train mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules and libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "# functions to create datasets and models\n",
    "from data.data import *\n",
    "from models.models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available\n"
     ]
    }
   ],
   "source": [
    "# check if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available')\n",
    "else:\n",
    "    print('CUDA is available')\n",
    "    device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Parameters for this experiment\"\"\"\n",
    "# params for loading the data\n",
    "path = 'data/Paderborn_FD/'\n",
    "source_domain = 'a'\n",
    "target_domains = ['b', 'c', 'd']\n",
    "\n",
    "# params for configuring the model\n",
    "input_dim = 1\n",
    "hidden_dim = 256\n",
    "output_dim = 3\n",
    "drop_prob = 0.5\n",
    "\n",
    "# params for training the model\n",
    "epochs_pre = 50 # pre-training\n",
    "epochs = 50\n",
    "\n",
    "# optimizing the model\n",
    "lr = 1e-4\n",
    "batch_size = 20\n",
    "beta1 = 0.5\n",
    "beta2 = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source domain data for training, validating and testing\n",
    "train_dataloader_src, val_dataloader_src, test_dataloader_src = generate_dataloaders(path, source_domain, batch_size)\n",
    "\n",
    "# target domain data for validating and testing\n",
    "train_dataloader_tgt, val_dataloader_tgt, test_dataloader_tgt = [], [], []\n",
    "for target_domain in target_domains:\n",
    "    train_dataloader_temp, val_dataloader_temp, test_dataloader_temp = generate_dataloaders(path, target_domain, batch_size)\n",
    "    \n",
    "    train_dataloader_tgt.append(train_dataloader_temp)\n",
    "    val_dataloader_tgt.append(val_dataloader_temp)\n",
    "    test_dataloader_tgt.append(test_dataloader_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_src = Encoder_AMDA(\n",
    "    input_dim=input_dim, # 1D input (sensor readings)\n",
    "    hidden_dim=hidden_dim # hidden layers 64\n",
    ")\n",
    "\n",
    "classifier = Classifier_AMDA(\n",
    "    hidden_dim=hidden_dim, # hidden layers 64\n",
    "    output_dim=output_dim, # 3 classes (healthy, inner- and outer-bearing damages)\n",
    "    dropout=drop_prob # dropout prob 0.5\n",
    ")\n",
    "\n",
    "encoder_tgt = Encoder_AMDA(\n",
    "    input_dim=input_dim, # 1D input (sensor readings)\n",
    "    hidden_dim=hidden_dim, # hidden layers 64\n",
    ")\n",
    "\n",
    "if train_on_gpu:\n",
    "    encoder_src = encoder_src.to(device)\n",
    "    classifier = classifier.to(device)\n",
    "    encoder_tgt = encoder_tgt.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions for training and testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for training source encoder and shared classifier\n",
    "def train_src(encoder, classifier):\n",
    "    \"\"\"Train the source encoder and shared classifier on source domain\"\"\"\n",
    "    encoder.train()\n",
    "    classifier.train()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(\n",
    "        list(encoder.parameters()) + list(classifier.parameters()),\n",
    "        lr=lr,\n",
    "        betas=(beta1, beta2)\n",
    "    )\n",
    "\n",
    "    for epoch in range(epochs_pre):\n",
    "        accuracies = []\n",
    "        losses = []\n",
    "\n",
    "        for batch_idx, (x_src, y_src) in enumerate(train_dataloader_src):\n",
    "            if train_on_gpu:\n",
    "                x_src, y_src = x_src.to(device), y_src.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            e_src = encoder(x_src)\n",
    "            pred_src = classifier(e_src)\n",
    "            loss_src = criterion(pred_src, y_src)\n",
    "            loss_src.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss_src.detach().item()) # detach the loss from compute graph \n",
    "            accuracies.append(y_src.eq(pred_src.detach().argmax(dim=1)).float().mean())\n",
    "        \n",
    "        # print for each epoch\n",
    "        print(f'Epoch: {epoch + 1} \\n \\t Train Loss:{torch.tensor(losses).mean():0.2f} \\t Train Acc:{torch.tensor(accuracies).mean():0.2f}')\n",
    "\n",
    "        # for each epoch, evaluate your model on the validation data\n",
    "        val_loss, val_acc = evaluate_src(encoder, classifier, phase = 'Val')\n",
    "        print(f'\\t Val Loss:{val_loss:0.2f} \\t\\t Val Acc:{val_acc:0.2f}')\n",
    "\n",
    "    return encoder, classifier\n",
    "\n",
    "# testing model on source domain\n",
    "def evaluate_src(encoder, classifier, phase='Val'): # phase can be validate or test\n",
    "    \"\"\"Evaluate the trained network on source domain\"\"\"\n",
    "    encoder.eval()\n",
    "    classifier.eval()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    dataloader = val_dataloader_src if phase == 'Val' else test_dataloader_src\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (x_src, y_src) in enumerate(dataloader):\n",
    "            if train_on_gpu:\n",
    "                x_src, y_src = x_src.to(device), y_src.to(device)\n",
    "\n",
    "            e_src = encoder(x_src)\n",
    "            pred_src = classifier(e_src)\n",
    "            loss_src = criterion(pred_src, y_src)\n",
    "            \n",
    "            # append loss for each batch\n",
    "            losses.append(loss_src.detach().item()) # detach the loss from compute graph \n",
    "            accuracies.append(y_src.eq(pred_src.detach().argmax(dim=1)).float().mean())\n",
    "            \n",
    "        mean_loss = torch.tensor(losses).mean()\n",
    "        mean_acc = torch.tensor(accuracies).mean()\n",
    "        return mean_loss, mean_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing model on target domains\n",
    "def evaluate_tgt(encoder, classifier, dataloader):\n",
    "    \"\"\"Evaluate the network on current target domain\"\"\"\n",
    "    encoder.train()\n",
    "    classifier.train()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (x, y) in enumerate(dataloader): # target dataloader\n",
    "            if(train_on_gpu):\n",
    "                x, y = x.to(device), y.to(device)\n",
    "\n",
    "            pred_tgt = classifier(encoder(x))\n",
    "            loss_tgt = criterion(pred_tgt, y)\n",
    "            \n",
    "            # append loss for each batch\n",
    "            losses.append(loss_tgt.detach().item()) # detach the loss from compute graph \n",
    "            accuracies.append(y.eq(pred_tgt.detach().argmax(dim=1)).float().mean())\n",
    "            \n",
    "        mean_loss = torch.tensor(losses).mean()\n",
    "        mean_acc = torch.tensor(accuracies).mean()\n",
    "        return mean_loss, mean_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \n",
      " \t Train Loss:0.81 \t Train Acc:0.56\n",
      "\t Val Loss:0.53 \t\t Val Acc:0.75\n",
      "Epoch: 2 \n",
      " \t Train Loss:0.46 \t Train Acc:0.80\n",
      "\t Val Loss:0.44 \t\t Val Acc:0.80\n",
      "Epoch: 3 \n",
      " \t Train Loss:0.41 \t Train Acc:0.81\n",
      "\t Val Loss:0.40 \t\t Val Acc:0.81\n",
      "Epoch: 4 \n",
      " \t Train Loss:0.33 \t Train Acc:0.83\n",
      "\t Val Loss:0.33 \t\t Val Acc:0.82\n",
      "Epoch: 5 \n",
      " \t Train Loss:0.21 \t Train Acc:0.91\n",
      "\t Val Loss:0.36 \t\t Val Acc:0.82\n",
      "Epoch: 6 \n",
      " \t Train Loss:0.16 \t Train Acc:0.93\n",
      "\t Val Loss:0.34 \t\t Val Acc:0.84\n",
      "Epoch: 7 \n",
      " \t Train Loss:0.12 \t Train Acc:0.95\n",
      "\t Val Loss:0.29 \t\t Val Acc:0.87\n",
      "Epoch: 8 \n",
      " \t Train Loss:0.09 \t Train Acc:0.97\n",
      "\t Val Loss:0.21 \t\t Val Acc:0.90\n",
      "Epoch: 9 \n",
      " \t Train Loss:0.07 \t Train Acc:0.97\n",
      "\t Val Loss:0.15 \t\t Val Acc:0.93\n",
      "Epoch: 10 \n",
      " \t Train Loss:0.06 \t Train Acc:0.98\n",
      "\t Val Loss:0.12 \t\t Val Acc:0.95\n",
      "Epoch: 11 \n",
      " \t Train Loss:0.06 \t Train Acc:0.98\n",
      "\t Val Loss:0.10 \t\t Val Acc:0.96\n",
      "Epoch: 12 \n",
      " \t Train Loss:0.05 \t Train Acc:0.98\n",
      "\t Val Loss:0.08 \t\t Val Acc:0.97\n",
      "Epoch: 13 \n",
      " \t Train Loss:0.05 \t Train Acc:0.98\n",
      "\t Val Loss:0.07 \t\t Val Acc:0.97\n",
      "Epoch: 14 \n",
      " \t Train Loss:0.04 \t Train Acc:0.99\n",
      "\t Val Loss:0.06 \t\t Val Acc:0.98\n",
      "Epoch: 15 \n",
      " \t Train Loss:0.04 \t Train Acc:0.99\n",
      "\t Val Loss:0.05 \t\t Val Acc:0.98\n",
      "Epoch: 16 \n",
      " \t Train Loss:0.04 \t Train Acc:0.99\n",
      "\t Val Loss:0.04 \t\t Val Acc:0.99\n",
      "Epoch: 17 \n",
      " \t Train Loss:0.03 \t Train Acc:0.99\n",
      "\t Val Loss:0.04 \t\t Val Acc:0.99\n",
      "Epoch: 18 \n",
      " \t Train Loss:0.03 \t Train Acc:0.99\n",
      "\t Val Loss:0.03 \t\t Val Acc:0.99\n",
      "Epoch: 19 \n",
      " \t Train Loss:0.03 \t Train Acc:0.99\n",
      "\t Val Loss:0.03 \t\t Val Acc:0.99\n",
      "Epoch: 20 \n",
      " \t Train Loss:0.03 \t Train Acc:0.99\n",
      "\t Val Loss:0.03 \t\t Val Acc:0.99\n",
      "Epoch: 21 \n",
      " \t Train Loss:0.03 \t Train Acc:0.99\n",
      "\t Val Loss:0.03 \t\t Val Acc:0.99\n",
      "Epoch: 22 \n",
      " \t Train Loss:0.02 \t Train Acc:0.99\n",
      "\t Val Loss:0.03 \t\t Val Acc:0.99\n",
      "Epoch: 23 \n",
      " \t Train Loss:0.02 \t Train Acc:0.99\n",
      "\t Val Loss:0.02 \t\t Val Acc:0.99\n",
      "Epoch: 24 \n",
      " \t Train Loss:0.02 \t Train Acc:0.99\n",
      "\t Val Loss:0.02 \t\t Val Acc:0.99\n",
      "Epoch: 25 \n",
      " \t Train Loss:0.02 \t Train Acc:0.99\n",
      "\t Val Loss:0.02 \t\t Val Acc:0.99\n",
      "Epoch: 26 \n",
      " \t Train Loss:0.02 \t Train Acc:0.99\n",
      "\t Val Loss:0.02 \t\t Val Acc:0.99\n",
      "Epoch: 27 \n",
      " \t Train Loss:0.02 \t Train Acc:0.99\n",
      "\t Val Loss:0.02 \t\t Val Acc:0.99\n",
      "Epoch: 28 \n",
      " \t Train Loss:0.02 \t Train Acc:0.99\n",
      "\t Val Loss:0.02 \t\t Val Acc:0.99\n",
      "Epoch: 29 \n",
      " \t Train Loss:0.02 \t Train Acc:0.99\n",
      "\t Val Loss:0.02 \t\t Val Acc:0.99\n",
      "Epoch: 30 \n",
      " \t Train Loss:0.02 \t Train Acc:0.99\n",
      "\t Val Loss:0.02 \t\t Val Acc:0.99\n",
      "Epoch: 31 \n",
      " \t Train Loss:0.01 \t Train Acc:1.00\n",
      "\t Val Loss:0.02 \t\t Val Acc:0.99\n",
      "Epoch: 32 \n",
      " \t Train Loss:0.01 \t Train Acc:1.00\n",
      "\t Val Loss:0.02 \t\t Val Acc:0.99\n",
      "Epoch: 33 \n",
      " \t Train Loss:0.01 \t Train Acc:1.00\n",
      "\t Val Loss:0.02 \t\t Val Acc:0.99\n",
      "Epoch: 34 \n",
      " \t Train Loss:0.01 \t Train Acc:1.00\n",
      "\t Val Loss:0.02 \t\t Val Acc:0.99\n",
      "Epoch: 35 \n",
      " \t Train Loss:0.01 \t Train Acc:1.00\n",
      "\t Val Loss:0.02 \t\t Val Acc:0.99\n",
      "Epoch: 36 \n",
      " \t Train Loss:0.01 \t Train Acc:1.00\n",
      "\t Val Loss:0.02 \t\t Val Acc:0.99\n",
      "Epoch: 37 \n",
      " \t Train Loss:0.01 \t Train Acc:1.00\n",
      "\t Val Loss:0.02 \t\t Val Acc:0.99\n",
      "Epoch: 38 \n",
      " \t Train Loss:0.01 \t Train Acc:1.00\n",
      "\t Val Loss:0.01 \t\t Val Acc:1.00\n",
      "Epoch: 39 \n",
      " \t Train Loss:0.01 \t Train Acc:1.00\n",
      "\t Val Loss:0.01 \t\t Val Acc:1.00\n",
      "Epoch: 40 \n",
      " \t Train Loss:0.01 \t Train Acc:1.00\n",
      "\t Val Loss:0.01 \t\t Val Acc:1.00\n",
      "Epoch: 41 \n",
      " \t Train Loss:0.01 \t Train Acc:1.00\n",
      "\t Val Loss:0.01 \t\t Val Acc:1.00\n",
      "Epoch: 42 \n",
      " \t Train Loss:0.01 \t Train Acc:1.00\n",
      "\t Val Loss:0.01 \t\t Val Acc:1.00\n",
      "Epoch: 43 \n",
      " \t Train Loss:0.01 \t Train Acc:1.00\n",
      "\t Val Loss:0.02 \t\t Val Acc:0.99\n",
      "Epoch: 44 \n",
      " \t Train Loss:0.01 \t Train Acc:1.00\n",
      "\t Val Loss:0.01 \t\t Val Acc:1.00\n",
      "Epoch: 45 \n",
      " \t Train Loss:0.01 \t Train Acc:1.00\n",
      "\t Val Loss:0.02 \t\t Val Acc:0.99\n",
      "Epoch: 46 \n",
      " \t Train Loss:0.01 \t Train Acc:1.00\n",
      "\t Val Loss:0.01 \t\t Val Acc:1.00\n",
      "Epoch: 47 \n",
      " \t Train Loss:0.01 \t Train Acc:1.00\n",
      "\t Val Loss:0.01 \t\t Val Acc:1.00\n",
      "Epoch: 48 \n",
      " \t Train Loss:0.01 \t Train Acc:1.00\n",
      "\t Val Loss:0.02 \t\t Val Acc:0.99\n",
      "Epoch: 49 \n",
      " \t Train Loss:0.01 \t Train Acc:1.00\n",
      "\t Val Loss:0.02 \t\t Val Acc:0.99\n",
      "Epoch: 50 \n",
      " \t Train Loss:0.01 \t Train Acc:1.00\n",
      "\t Val Loss:0.02 \t\t Val Acc:0.99\n"
     ]
    }
   ],
   "source": [
    "# pre-training\n",
    "encoder_src, classifier = train_src(encoder_src, classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous Unsupervised Adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Phase 1 (Domain a to Domain b)\n",
      "\n",
      "Testing Accuracy on Previously Seen and Current Domains:\n",
      "Source Domain a:\n",
      "\t Test (with Source) Loss:0.03 \t Test (with Source) Acc:0.99\n",
      "Target Domain b:\n",
      "\t Test (with Target) Loss:24.67 \t Test (with Target) Acc:0.34\n",
      "\n",
      "Phase 2 (Domain b to Domain c)\n",
      "\n",
      "Testing Accuracy on Previously Seen and Current Domains:\n",
      "Source Domain a:\n",
      "\t Test (with Source) Loss:0.03 \t Test (with Source) Acc:0.99\n",
      "Target Domain b:\n",
      "\t Test (with Target) Loss:16.74 \t Test (with Target) Acc:0.47\n",
      "Target Domain c:\n",
      "\t Test (with Target) Loss:16.66 \t Test (with Target) Acc:0.46\n",
      "\n",
      "Phase 3 (Domain c to Domain d)\n",
      "\n",
      "Testing Accuracy on Previously Seen and Current Domains:\n",
      "Source Domain a:\n",
      "\t Test (with Source) Loss:0.03 \t Test (with Source) Acc:0.99\n",
      "Target Domain b:\n",
      "\t Test (with Target) Loss:20.38 \t Test (with Target) Acc:0.34\n",
      "Target Domain c:\n",
      "\t Test (with Target) Loss:20.40 \t Test (with Target) Acc:0.34\n",
      "Target Domain d:\n",
      "\t Test (with Target) Loss:20.79 \t Test (with Target) Acc:0.34\n"
     ]
    }
   ],
   "source": [
    "encoder_tgt.load_state_dict(encoder_src.state_dict())\n",
    "\n",
    "for phase in range(len(target_domains)):\n",
    "    if(phase == 0): # Domain A to B\n",
    "        print(f'\\nPhase {phase + 1} (Domain {source_domain} to Domain {target_domains[phase]})')\n",
    "    else: # Domain B to C, and Domain C to D\n",
    "        print(f'\\nPhase {phase + 1} (Domain {target_domains[phase - 1]} to Domain {target_domains[phase]})')\n",
    "\n",
    "    print('\\nTesting Accuracy on Previously Seen and Current Domains:')\n",
    "    print(f'Source Domain {source_domain}:')\n",
    "    src_test_loss, src_test_acc = evaluate_src(encoder_tgt, classifier, phase='Test')\n",
    "    print(f'\\t Test (with Source) Loss:{src_test_loss:0.2f} \\t Test (with Source) Acc:{src_test_acc:0.2f}') \n",
    "    for p in range(phase + 1):\n",
    "        print(f'Target Domain {target_domains[p]}:')\n",
    "        tgt_test_loss, tgt_test_acc = evaluate_tgt(encoder_tgt, classifier, test_dataloader_tgt[phase])\n",
    "        print(f'\\t Test (with Target) Loss:{tgt_test_loss:0.2f} \\t Test (with Target) Acc:{tgt_test_acc:0.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('deep-learning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dbbf5940fb39bd3f383e6d88b474dca1e13ffd10acf1bdc8d3732bf030dd4f5c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
